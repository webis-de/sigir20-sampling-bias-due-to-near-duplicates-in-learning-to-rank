{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from math import sqrt\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Workspace Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_DIR = os.path.expanduser(\"~/wstud-thesis-reimer/notebooks/\")\n",
    "EVALUATION_DIR = WORKSPACE_DIR + \"2020-05-06-04-12/\"\n",
    "OUTPUT_DIR = WORKSPACE_DIR + \"figures/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_names = {\n",
    "    \"map-per-topic\": r\"\\map{}~Performance\",\n",
    "    \"ndcg-per-topic\": r\"\\ndcg{}~Performance\",\n",
    "    \"ndcg@10-per-topic\": r\"\\ndcg{10}~Performance\",\n",
    "    \"ndcg@20-per-topic\": r\"\\ndcg{20}~Performance\",\n",
    "    \"first-wikipedia-rank-per-topic\": r\"Mean First Rank of Wikipedia Documents\",\n",
    "    \"first-irrelevant-wikipedia-rank-per-topic\": r\"Mean First Rank of Irrelevant Wikipedia Documents\",\n",
    "    \"domain-fairness-per-topic\": r\"Fairness of Exposure Across Domains\"\n",
    "}\n",
    "\n",
    "corpus_names = {\n",
    "    \"clueweb09\": r\"ClueWeb~09\",\n",
    "    \"gov2\": r\"GOV2\"\n",
    "}\n",
    "\n",
    "run_sampling_names = {\n",
    "    \"identity\": r\"Duplicates Unmodified\",\n",
    "    \"duplicates-irrelevant\": r\"Duplicates Irrelevant\",\n",
    "    \"remove-duplicates\": r\"Duplicates Removed\"\n",
    "}\n",
    "\n",
    "ranker_names = {\n",
    "    \"bm25\": r\"BM25\",\n",
    "    \"ada-rank\": r\"AdaRank\",\n",
    "    \"coordinate-ascent\": r\"Coor.~Ascent\",\n",
    "    \"lambda-mart\": r\"LambdaMART\",\n",
    "    \"list-net\": r\"ListNET\",\n",
    "    \"rank-boost\": r\"RankBoost\",\n",
    "    \"linear-regression\": r\"Regression\"\n",
    "}\n",
    "\n",
    "sampling_names = {\n",
    "    (\"identity\", \"identity\", \"identity\"): r\"100\\,\\%\",\n",
    "    (\"no-wikipedia-redundancy\", \"identity\", \"identity\"): r\"0\\,\\%\",\n",
    "#     (\"identity\", \"identity\", \"novelty-relevance-feedback-null\"): r\"NOV\\textsubscript{0}\",\n",
    "#     (\"identity\", \"identity\", \"novelty-relevance-feedback-null-novelty-feature\"): r\"NOV\\textsubscript{0,F}\",\n",
    "#     (\"identity\", \"identity\", \"novelty-relevance-feedback-scale\"): r\"NOV\\textsubscript{S}\",\n",
    "    (\"identity\", \"identity\", \"novelty-relevance-feedback-scale-novelty-feature\"): r\"NOV\\textsubscript{S,F}\",\n",
    "#     (\"no-wikipedia-redundancy\", \"identity\", \"novelty-relevance-feedback-null\"): \"\",\n",
    "#     (\"no-wikipedia-redundancy\", \"identity\", \"novelty-relevance-feedback-null-novelty-feature\"): \"\",\n",
    "#     (\"no-wikipedia-redundancy\", \"identity\", \"novelty-relevance-feedback-scale\"): \"\",\n",
    "#     (\"no-wikipedia-redundancy\", \"identity\", \"novelty-relevance-feedback-scale-novelty-feature\"): \"\"\n",
    "}\n",
    "\n",
    "split_names = {\n",
    "    \"most-redundant-training\": r\"Worst-Case Scenario\",\n",
    "#     \"3-fold-cross-validation-1\": r\"3-Fold Cross Validation\",\n",
    "#     \"3-fold-cross-validation-2\": r\"3-Fold Cross Validation\",\n",
    "#     \"3-fold-cross-validation-3\": r\"3-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-1\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-2\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-3\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-4\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-5\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-mostredundanttraining\": r\"Worst-Case Scenario\",\n",
    "    \"clueweb09-fold1\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold2\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold3\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold4\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold5\": r\"5-Fold Cross Validation\",\n",
    "    \"letor-trec-millionquery2007-fold-1\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"letor-trec-millionquery2007-fold-2\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"letor-trec-millionquery2007-fold-3\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"letor-trec-millionquery2007-fold-4\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"letor-trec-millionquery2007-fold-5\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"letor-trec-millionquery2008-fold-1\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"letor-trec-millionquery2008-fold-2\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"letor-trec-millionquery2008-fold-3\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"letor-trec-millionquery2008-fold-4\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"letor-trec-millionquery2008-fold-5\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"trec-millionquery2007-fold1\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"trec-millionquery2007-fold2\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"trec-millionquery2007-fold3\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"trec-millionquery2007-fold4\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"trec-millionquery2007-fold5\": r\"5-Fold Cross Validation MQ2007\",\n",
    "    \"trec-millionquery2008-fold1\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"trec-millionquery2008-fold2\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"trec-millionquery2008-fold3\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"trec-millionquery2008-fold4\": r\"5-Fold Cross Validation MQ2008\",\n",
    "    \"trec-millionquery2008-fold5\": r\"5-Fold Cross Validation MQ2008\"\n",
    "}\n",
    "\n",
    "evaluation_filter_metrics = {\n",
    "    \"map-per-topic\": \"ndcg@20\",\n",
    "    \"ndcg-per-topic\": \"ndcg@20\",\n",
    "    \"ndcg@10-per-topic\": \"ndcg@10\",\n",
    "    \"ndcg@20-per-topic\": \"ndcg@20\",\n",
    "    \"first-wikipedia-rank-per-topic\": \"ndcg@20\",\n",
    "    \"first-irrelevant-wikipedia-rank-per-topic\": \"ndcg@20\",\n",
    "    \"domain-fairness-per-topic\": \"ndcg@20\"\n",
    "}\n",
    "\n",
    "evaluations = list(evaluation_names.keys())\n",
    "\n",
    "corpora = list(corpus_names.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "baseline_ranker = \"BM25\"\n",
    "baseline_sampling = sampling_names[(\"identity\", \"identity\", \"identity\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse evaluation data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>metric</th>\n",
       "      <th>underSampling</th>\n",
       "      <th>overSampling</th>\n",
       "      <th>featureMutation</th>\n",
       "      <th>trial</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>test-set-result</th>\n",
       "      <th>train-set-result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clueweb09</td>\n",
       "      <td>3-fold-cross-validation-1</td>\n",
       "      <td>linear-regression</td>\n",
       "      <td>ndcg@20</td>\n",
       "      <td>no-wikipedia-redundancy</td>\n",
       "      <td>identity</td>\n",
       "      <td>novelty-relevance-feedback-null</td>\n",
       "      <td>trial-0</td>\n",
       "      <td>remove-duplicates</td>\n",
       "      <td>map-per-topic</td>\n",
       "      <td>[0.0, 0.579477365130462, 0.41082469625746004, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clueweb09</td>\n",
       "      <td>3-fold-cross-validation-1</td>\n",
       "      <td>linear-regression</td>\n",
       "      <td>ndcg@20</td>\n",
       "      <td>no-wikipedia-redundancy</td>\n",
       "      <td>identity</td>\n",
       "      <td>novelty-relevance-feedback-null</td>\n",
       "      <td>trial-0</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>map-per-topic</td>\n",
       "      <td>[0.0, 0.575660349788628, 0.402742626488414, 0....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39898</th>\n",
       "      <td>gov2</td>\n",
       "      <td>letor-trec-millionquery2008-fold-5</td>\n",
       "      <td>list-net</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>trial-3</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>map-per-topic</td>\n",
       "      <td>[0.30952380952380903, 0.25, 0.7000000000000001...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39899</th>\n",
       "      <td>gov2</td>\n",
       "      <td>letor-trec-millionquery2008-fold-5</td>\n",
       "      <td>list-net</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>trial-3</td>\n",
       "      <td>identity</td>\n",
       "      <td>map-per-topic</td>\n",
       "      <td>[0.30952380952380903, 0.25, 0.7000000000000001...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39900 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          corpus                      trainTestSplit             ranker  \\\n",
       "0      clueweb09           3-fold-cross-validation-1  linear-regression   \n",
       "1      clueweb09           3-fold-cross-validation-1  linear-regression   \n",
       "...          ...                                 ...                ...   \n",
       "39898       gov2  letor-trec-millionquery2008-fold-5           list-net   \n",
       "39899       gov2  letor-trec-millionquery2008-fold-5           list-net   \n",
       "\n",
       "        metric            underSampling overSampling  \\\n",
       "0      ndcg@20  no-wikipedia-redundancy     identity   \n",
       "1      ndcg@20  no-wikipedia-redundancy     identity   \n",
       "...        ...                      ...          ...   \n",
       "39898     ndcg                 identity     identity   \n",
       "39899     ndcg                 identity     identity   \n",
       "\n",
       "                       featureMutation    trial            runSampling  \\\n",
       "0      novelty-relevance-feedback-null  trial-0      remove-duplicates   \n",
       "1      novelty-relevance-feedback-null  trial-0  duplicates-irrelevant   \n",
       "...                                ...      ...                    ...   \n",
       "39898                         identity  trial-3  duplicates-irrelevant   \n",
       "39899                         identity  trial-3               identity   \n",
       "\n",
       "          evaluation                                    test-set-result  \\\n",
       "0      map-per-topic  [0.0, 0.579477365130462, 0.41082469625746004, ...   \n",
       "1      map-per-topic  [0.0, 0.575660349788628, 0.402742626488414, 0....   \n",
       "...              ...                                                ...   \n",
       "39898  map-per-topic  [0.30952380952380903, 0.25, 0.7000000000000001...   \n",
       "39899  map-per-topic  [0.30952380952380903, 0.25, 0.7000000000000001...   \n",
       "\n",
       "       train-set-result  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "...                 ...  \n",
       "39898               NaN  \n",
       "39899               NaN  \n",
       "\n",
       "[39900 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from JSON-Lines file.\n",
    "def get_evaluation_raw(name):\n",
    "    file = EVALUATION_DIR + \"evaluation-of-experiments-\" + name + \".jsonl\"\n",
    "    return pd.read_json(open(file), lines=True)\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation_raw(evaluations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>test-set-result</th>\n",
       "      <th>sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-fold-cross-validation-1</td>\n",
       "      <td>linear-regression</td>\n",
       "      <td>remove-duplicates</td>\n",
       "      <td>[0.0, 0.579477365130462, 0.41082469625746004, ...</td>\n",
       "      <td>(no-wikipedia-redundancy, identity, novelty-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3-fold-cross-validation-1</td>\n",
       "      <td>linear-regression</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>[0.0, 0.575660349788628, 0.402742626488414, 0....</td>\n",
       "      <td>(no-wikipedia-redundancy, identity, novelty-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18748</th>\n",
       "      <td>5-fold-cross-validation-3</td>\n",
       "      <td>list-net</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>[0.13147977116470402, 0.44935388571531404, 0.4...</td>\n",
       "      <td>(identity, identity, identity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18749</th>\n",
       "      <td>5-fold-cross-validation-3</td>\n",
       "      <td>list-net</td>\n",
       "      <td>identity</td>\n",
       "      <td>[0.09949870040000401, 0.46063412773524504, 0.5...</td>\n",
       "      <td>(identity, identity, identity)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9450 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  trainTestSplit             ranker            runSampling  \\\n",
       "0      3-fold-cross-validation-1  linear-regression      remove-duplicates   \n",
       "1      3-fold-cross-validation-1  linear-regression  duplicates-irrelevant   \n",
       "...                          ...                ...                    ...   \n",
       "18748  5-fold-cross-validation-3           list-net  duplicates-irrelevant   \n",
       "18749  5-fold-cross-validation-3           list-net               identity   \n",
       "\n",
       "                                         test-set-result  \\\n",
       "0      [0.0, 0.579477365130462, 0.41082469625746004, ...   \n",
       "1      [0.0, 0.575660349788628, 0.402742626488414, 0....   \n",
       "...                                                  ...   \n",
       "18748  [0.13147977116470402, 0.44935388571531404, 0.4...   \n",
       "18749  [0.09949870040000401, 0.46063412773524504, 0.5...   \n",
       "\n",
       "                                                sampling  \n",
       "0      (no-wikipedia-redundancy, identity, novelty-re...  \n",
       "1      (no-wikipedia-redundancy, identity, novelty-re...  \n",
       "...                                                  ...  \n",
       "18748                     (identity, identity, identity)  \n",
       "18749                     (identity, identity, identity)  \n",
       "\n",
       "[9450 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_evaluation(evaluation_name, corpus=None):\n",
    "    evaluation = get_evaluation_raw(evaluation_name)\n",
    "    \n",
    "    # Drop training set results.\n",
    "    evaluation = evaluation.drop(columns=[\"train-set-result\"])\n",
    "\n",
    "    # Drop evaluation column.\n",
    "    evaluation = evaluation.drop(columns=[\"evaluation\"])\n",
    "\n",
    "    # Drop trial column.\n",
    "    evaluation = evaluation.drop(columns=[\"trial\"])\n",
    "\n",
    "    # Filter corpus.\n",
    "    if corpus:\n",
    "        evaluation = evaluation[evaluation[\"corpus\"] == corpus]\\\n",
    "            .drop(columns=[\"corpus\"])\n",
    "\n",
    "    # Filter models with metric.\n",
    "    filter_metric = evaluation_filter_metrics[evaluation_name]\n",
    "    evaluation = evaluation[evaluation[\"metric\"] == filter_metric]\\\n",
    "        .drop(columns=[\"metric\"])\n",
    "\n",
    "    # Merge samplings into one column.\n",
    "    evaluation[\"sampling\"] = evaluation[[\"underSampling\",\"overSampling\",\"featureMutation\"]]\\\n",
    "        .aggregate(tuple, axis=1)\n",
    "    evaluation = evaluation.drop(columns=[\"underSampling\",\"overSampling\",\"featureMutation\"])\n",
    "\n",
    "    return evaluation\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation(evaluations[0], corpora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>test-set-result</th>\n",
       "      <th>sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Removed</td>\n",
       "      <td>[0.40674294973964203, 0.22844662619741102, 0.3...</td>\n",
       "      <td>0\\,\\%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Irrelevant</td>\n",
       "      <td>[0.399421748903214, 0.226415002978672, 0.31319...</td>\n",
       "      <td>0\\,\\%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18148</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>RankBoost</td>\n",
       "      <td>Duplicates Irrelevant</td>\n",
       "      <td>[0.135521983373488, 0.609188010319184, 0.61538...</td>\n",
       "      <td>100\\,\\%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>RankBoost</td>\n",
       "      <td>Duplicates Unmodified</td>\n",
       "      <td>[0.076045925713964, 0.555690177523385, 0.79104...</td>\n",
       "      <td>100\\,\\%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                trainTestSplit      ranker            runSampling  \\\n",
       "2160   5-Fold Cross Validation  Regression     Duplicates Removed   \n",
       "2161   5-Fold Cross Validation  Regression  Duplicates Irrelevant   \n",
       "...                        ...         ...                    ...   \n",
       "18148  5-Fold Cross Validation   RankBoost  Duplicates Irrelevant   \n",
       "18149  5-Fold Cross Validation   RankBoost  Duplicates Unmodified   \n",
       "\n",
       "                                         test-set-result sampling  \n",
       "2160   [0.40674294973964203, 0.22844662619741102, 0.3...    0\\,\\%  \n",
       "2161   [0.399421748903214, 0.226415002978672, 0.31319...    0\\,\\%  \n",
       "...                                                  ...      ...  \n",
       "18148  [0.135521983373488, 0.609188010319184, 0.61538...  100\\,\\%  \n",
       "18149  [0.076045925713964, 0.555690177523385, 0.79104...  100\\,\\%  \n",
       "\n",
       "[1350 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_evaluation_labeled(evaluation_name, corpus=None):\n",
    "    evaluation = get_evaluation(evaluation_name, corpus)\n",
    "\n",
    "    # Map names.\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        evaluation[\"corpus\"] = evaluation[\"corpus\"].map(lambda split : corpus_names.get(split, \"\"))\n",
    "    evaluation[\"trainTestSplit\"] = evaluation[\"trainTestSplit\"].map(lambda split : split_names.get(split, \"\"))\n",
    "    evaluation[\"ranker\"] = evaluation[\"ranker\"].map(lambda ranker : ranker_names.get(ranker, \"\"))\n",
    "    evaluation[\"runSampling\"] = evaluation[\"runSampling\"].map(lambda run_sampling : run_sampling_names.get(run_sampling, \"\"))\n",
    "    evaluation[\"sampling\"] = evaluation[\"sampling\"].map(lambda sampling : sampling_names.get(sampling, \"\"))\n",
    "\n",
    "    # Filter empty (ignored) names.\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        evaluation=evaluation[evaluation[\"corpus\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"trainTestSplit\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"ranker\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"runSampling\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"sampling\"] != \"\"]\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation_labeled(evaluations[0], corpora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>sampling</th>\n",
       "      <th>test-set-result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Worst-Case Scenario</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Duplicates Unmodified</td>\n",
       "      <td>100\\,\\%</td>\n",
       "      <td>[0.181763590354475, 0.0, 0.22351291569622303, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst-Case Scenario</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Duplicates Unmodified</td>\n",
       "      <td>0\\,\\%</td>\n",
       "      <td>[0.181763590354475, 0.0, 0.22351291569622303, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Removed</td>\n",
       "      <td>0\\,\\%</td>\n",
       "      <td>[0.40674294973964203, 0.22844662619741102, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Removed</td>\n",
       "      <td>NOV\\textsubscript{S,F}</td>\n",
       "      <td>[0.413908825975192, 0.26189286610338103, 0.328...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             trainTestSplit      ranker            runSampling  \\\n",
       "0       Worst-Case Scenario        BM25  Duplicates Unmodified   \n",
       "1       Worst-Case Scenario        BM25  Duplicates Unmodified   \n",
       "..                      ...         ...                    ...   \n",
       "88  5-Fold Cross Validation  Regression     Duplicates Removed   \n",
       "89  5-Fold Cross Validation  Regression     Duplicates Removed   \n",
       "\n",
       "                  sampling                                    test-set-result  \n",
       "0                  100\\,\\%  [0.181763590354475, 0.0, 0.22351291569622303, ...  \n",
       "1                    0\\,\\%  [0.181763590354475, 0.0, 0.22351291569622303, ...  \n",
       "..                     ...                                                ...  \n",
       "88                   0\\,\\%  [0.40674294973964203, 0.22844662619741102, 0.3...  \n",
       "89  NOV\\textsubscript{S,F}  [0.413908825975192, 0.26189286610338103, 0.328...  \n",
       "\n",
       "[90 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorical_type(categories):\n",
    "    categories = list(categories)\n",
    "    categories = sorted(set(categories), key=categories.index)\n",
    "    return pd.api.types.CategoricalDtype(categories=categories, ordered=True)\n",
    "\n",
    "# Categories:\n",
    "corpus_categorical_type = categorical_type(corpus_names.values())\n",
    "split_categorical_type = categorical_type(split_names.values())\n",
    "ranker_categorical_type = categorical_type(ranker_names.values())\n",
    "run_sampling_categorical_type = categorical_type(run_sampling_names.values())\n",
    "sampling_categorical_type = categorical_type(sampling_names.values())\n",
    "\n",
    "def get_evaluation_aggregated(evaluation_name, corpus=None):\n",
    "    evaluation = get_evaluation_labeled(evaluation_name, corpus)\n",
    "\n",
    "    # Make types categorical.\n",
    "    types = {\n",
    "        \"trainTestSplit\": split_categorical_type,\n",
    "        \"ranker\": ranker_categorical_type,\n",
    "        \"runSampling\": run_sampling_categorical_type,\n",
    "        \"sampling\": sampling_categorical_type\n",
    "    }\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        types.update({\"corpus\" : corpus_categorical_type})\n",
    "    evaluation = evaluation.astype(types)\n",
    "    \n",
    "    # Sort.\n",
    "    sort_cols = [\"trainTestSplit\", \"ranker\", \"runSampling\", \"sampling\"]\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        sort_cols.insert(0, \"corpus\")\n",
    "    evaluation = evaluation.sort_values(by=sort_cols)\n",
    "\n",
    "    # Aggregate trials.\n",
    "    evaluation = evaluation.groupby(sort_cols)\\\n",
    "        .aggregate(lambda lists : [item for sublist in lists for item in sublist])\\\n",
    "        .dropna()\\\n",
    "        .reset_index()\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation_aggregated(evaluations[0], corpora[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_P_VALUE = 0.05\n",
    "\n",
    "def significantly_better(compare, baseline):\n",
    "    test = ttest_ind(compare,baseline)\n",
    "    return test.statistic > 0 and test.pvalue <= MAX_P_VALUE\n",
    "\n",
    "def cohens_d(compare, baseline):\n",
    "    return (mean(compare) - mean(baseline)) / (sqrt((stdev(compare) ** 2 + stdev(baseline) ** 2) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LaTeX table from data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_columns(n):\n",
    "    return [\"\"] * n\n",
    "\n",
    "def table(name, corpus=None, decimals=3):\n",
    "    evaluation = get_evaluation_aggregated(name, corpus)\n",
    "    \n",
    "    rankers = evaluation[\"ranker\"].unique()\n",
    "    run_samplings = evaluation[\"runSampling\"].unique()\n",
    "    samplings = evaluation[\"sampling\"].unique()\n",
    "\n",
    "\n",
    "    def table_head():\n",
    "        if not corpus:\n",
    "            head = [\"Corpus\", \"Split\", \"Algorithm\"]\n",
    "        else: \n",
    "            head = [\"Split\", \"Algorithm\"]\n",
    "        head.append(evaluation_names[name])\n",
    "        head += empty_columns(len(samplings) * len(run_samplings) - 1)\n",
    "        head = list(map(lambda item : r\"\\textbf{\" + item + r\"}\" if len(item) > 0 else item, head))\n",
    "        return head\n",
    "    \n",
    "    \n",
    "    def table_subhead():\n",
    "        head = empty_columns(3 if not corpus else 2)\n",
    "        for run_sampling in run_samplings:\n",
    "            head.append(run_sampling)\n",
    "            head += empty_columns(len(samplings) - 1)\n",
    "        return head\n",
    "    \n",
    "    \n",
    "    def table_subsubhead():\n",
    "        head = empty_columns(3 if not corpus else 2)\n",
    "        for _ in run_samplings:\n",
    "            for sampling in samplings:\n",
    "                head.append(sampling)\n",
    "        return head\n",
    "\n",
    "\n",
    "    def table_cell(baseline, compare):\n",
    "        column = r\"\\(\"\n",
    "\n",
    "        significant = significantly_better(compare, baseline)\n",
    "        if significant:\n",
    "            column += r\"\\mathbf{\"\n",
    "\n",
    "        column += (\"{:.\" + str(decimals) + \"f}\").format(mean(compare))\n",
    "\n",
    "        d = cohens_d(compare, baseline)\n",
    "        if d > 0:\n",
    "            column += r\"\\updiff{\"\n",
    "            column += \"{:.2f}\".format(d)\n",
    "            column += r\"}\"\n",
    "        elif d < 0:\n",
    "            column += r\"\\downdiff{\"\n",
    "            column += \"{:.2f}\".format(-d)\n",
    "            column += r\"}\"\n",
    "        else:\n",
    "            column += r\"\\nodiff{\"\n",
    "            column += \"{:.2f}\".format(d)\n",
    "            column += r\"}\"\n",
    "\n",
    "        if significant:\n",
    "            column += r\"}\"\n",
    "\n",
    "        column += r\"\\)\"\n",
    "        return column\n",
    "\n",
    "\n",
    "    def table_row(split, split_tex, ranker, row_corpus=None):\n",
    "        if row_corpus:\n",
    "            row = [row_corpus, split_tex, ranker]\n",
    "        else:\n",
    "            row = [split_tex, ranker]\n",
    "        for run_sampling in run_samplings:\n",
    "            df = evaluation\n",
    "            if row_corpus:\n",
    "                df = df[df[\"corpus\"] == row_corpus]\n",
    "            df = df[df[\"trainTestSplit\"] == split]\n",
    "            df = df[df[\"ranker\"] == ranker]\n",
    "            df = df[df[\"runSampling\"] == run_sampling]\n",
    "            if row_corpus:\n",
    "                drop_columns = [\"corpus\", \"trainTestSplit\", \"ranker\", \"runSampling\"]\n",
    "            else: \n",
    "                drop_columns = [\"trainTestSplit\", \"ranker\", \"runSampling\"]\n",
    "            df = df.drop(columns=drop_columns)\n",
    "            baseline_result = df[df[\"sampling\"] == baseline_sampling][\"test-set-result\"].iloc[0]\n",
    "            \n",
    "            row.append(r\"\\(\" + (\"{:.\" + str(decimals) + \"f}\").format(mean(baseline_result)) + r\"\\)\")\n",
    "            for sampling in samplings:\n",
    "                if sampling != baseline_sampling:\n",
    "                    if ranker == baseline_ranker:\n",
    "                        # We don't see sampling differences in BM25 Ranking, \n",
    "                        # as those don't depend on training data.\n",
    "                        # Therefore hide all except the first.\n",
    "                        row.append(r\"---\")\n",
    "                    else:\n",
    "                        compare_result = df[df[\"sampling\"] == sampling][\"test-set-result\"].iloc[0]\n",
    "                        row.append(table_cell(baseline_result, compare_result))\n",
    "        return row\n",
    "\n",
    "\n",
    "    def table_rows():\n",
    "        def split_rotated(split_name, num_rankers):\n",
    "            return r\"\\multirow{\" + str(num_rankers) +\\\n",
    "                r\"}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{\" +\\\n",
    "                str(num_rankers + 1) +\\\n",
    "                r\"em}{\\centering \\textbf{\" + split_name + \"}}}}\"\n",
    "        \n",
    "        rows = []\n",
    "        if not corpus:\n",
    "            for corp in evaluation[\"corpus\"].unique():\n",
    "                corpus_df = evaluation[evaluation[\"corpus\"] == corp]\n",
    "                for split in corpus_df[\"trainTestSplit\"].unique():\n",
    "                    split_tex = split_rotated(split, len(rankers))\n",
    "                    for ranker in rankers:\n",
    "                        rows.append(table_row(split, split_tex, ranker, corp))\n",
    "                        split_tex = \"\"\n",
    "        else:\n",
    "            for split in evaluation[\"trainTestSplit\"].unique():\n",
    "                split_tex = split_rotated(split, len(rankers))\n",
    "                for ranker in rankers:\n",
    "                    rows.append(table_row(split, split_tex, ranker))\n",
    "                    split_tex = \"\"\n",
    "        return rows\n",
    "\n",
    "\n",
    "    table_data = [\n",
    "        table_head(),\n",
    "        table_subhead(),\n",
    "        table_subsubhead()\n",
    "    ] + table_rows()\n",
    "\n",
    "    return tabulate(table_data, tablefmt=\"latex_raw\")\n",
    "\n",
    "def write_table(evaluation, corpus=None, decimals=3):\n",
    "    file_name = OUTPUT_DIR + (corpus + \"-\" if corpus else \"\") + evaluation + '.tex'\n",
    "    with codecs.open(file_name, 'w', 'utf-8') as file:\n",
    "        content = table(evaluation, corpus, decimals)\n",
    "        content = re.sub(r\"\\s+&\\s+\", \" & \",content)\n",
    "        content = re.sub(r\"\\s+\\\\\\\\\", r\" \\\\\\\\\",content)\n",
    "        file.write(r\"\\documentclass[preview]{standalone}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\ndcg}[1]{nDCG\\def\\tempndcg{#1}\\ifx\\tempndcg\\empty\\else{@}\\tempndcg\\fi}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\map}{MAP}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\updiff}[1]{^{\\text{↑}#1}}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\downdiff}[1]{^{\\text{↓}#1}}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\nodiff}[1]{^{\\text{=}#1}}\" + \"\\n\" +\\\n",
    "                   r\"\\begin{document}\" + \"\\n\")\n",
    "        file.write(content)\n",
    "        file.write(r\"\\end{document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table(\"domain-fairness-per-topic\")\n",
    "write_table(\"domain-fairness-per-topic\", corpus=\"gov2\")\n",
    "write_table(\"domain-fairness-per-topic\", corpus=\"clueweb09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table(\"map-per-topic\")\n",
    "# write_table(\"map-per-topic\", corpus=\"gov2\")\n",
    "# write_table(\"map-per-topic\", corpus=\"clueweb09\")\n",
    "# write_table(\"ndcg@10-per-topic\")\n",
    "# write_table(\"ndcg@10-per-topic\", corpus=\"gov2\")\n",
    "# write_table(\"ndcg@10-per-topic\", corpus=\"clueweb09\")\n",
    "# write_table(\"ndcg@20-per-topic\")\n",
    "write_table(\"ndcg@20-per-topic\", corpus=\"gov2\")\n",
    "write_table(\"ndcg@20-per-topic\", corpus=\"clueweb09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table(\"first-wikipedia-rank-per-topic\", decimals=0, corpus=\"clueweb09\")\n",
    "write_table(\"first-irrelevant-wikipedia-rank-per-topic\", decimals=0, corpus=\"clueweb09\")\n",
    "# write_table(\"first-duplicate-rank-per-topic\", decimals=0)\n",
    "# write_table(\"first-duplicate-rank-per-topic\", decimals=0, corpus=\"gov2\")\n",
    "# write_table(\"first-duplicate-rank-per-topic\", decimals=0, corpus=\"clueweb09\")\n",
    "# write_table(\"first-irrelevant-duplicate-rank-per-topic\", decimals=0)\n",
    "write_table(\"first-irrelevant-duplicate-rank-per-topic\", decimals=0, corpus=\"gov2\")\n",
    "write_table(\"first-irrelevant-duplicate-rank-per-topic\", decimals=0, corpus=\"clueweb09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}