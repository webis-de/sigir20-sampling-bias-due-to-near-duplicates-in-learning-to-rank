{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import codecs\n",
    "import re\n",
    "from math import sqrt\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Workspace Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Heinrich/Development/uni-halle-big-data/sigir20-sampling-bias-due-to-near-duplicates-in-learning-to-rank/data/figures')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIMESTAMP = \"2020-05-06-04-13\"\n",
    "\n",
    "NOTEBOOKS_DIR = Path(os.getcwd())\n",
    "WORKSPACE_DIR = NOTEBOOKS_DIR.parent\n",
    "DATA_DIR = WORKSPACE_DIR / \"data\"\n",
    "EVALUATION_DIR = (DATA_DIR / \"experiments\" / TIMESTAMP).resolve(True)\n",
    "OUTPUT_DIR = (DATA_DIR / \"figures\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_names = {\n",
    "    \"ndcg@10-per-topic\": r\"\\ndcg{10}~Performance\",\n",
    "    \"ndcg@20-per-topic\": r\"\\ndcg{20}~Performance\",\n",
    "    \"ndcg-per-topic\": r\"\\ndcg{}~Performance\",\n",
    "    \"map-per-topic\": r\"\\map{}~Performance\",\n",
    "    \"first-wikipedia-rank-per-topic\": r\"Mean First Rank of Wikipedia Documents\",\n",
    "    \"first-irrelevant-wikipedia-rank-per-topic\": r\"Mean First Rank of Irrelevant Wikipedia Documents\",\n",
    "    \"first-duplicate-rank-per-topic\": r\"Mean First Rank of Wikipedia Documents\",\n",
    "    \"first-irrelevant-duplicate-rank-per-topic\": r\"Mean First Rank of Irrelevant Wikipedia Documents\",\n",
    "    \"domain-fairness-per-topic\": r\"Fairness of Exposure Across Domains\"\n",
    "}\n",
    "\n",
    "corpus_names = {\n",
    "    \"clueweb09\": r\"ClueWeb~09\",\n",
    "    \"gov2\": r\"GOV2\"\n",
    "}\n",
    "\n",
    "run_sampling_names = {\n",
    "    \"identity\": r\"Duplicates Unmodified\",\n",
    "    \"duplicates-irrelevant\": r\"Duplicates Irrelevant\",\n",
    "    \"remove-duplicates\": r\"Duplicates Removed\"\n",
    "}\n",
    "\n",
    "ranker_names = {\n",
    "    \"bm25\": r\"BM25\",\n",
    "    \"ada-rank\": r\"AdaRank\",\n",
    "    \"coordinate-ascent\": r\"Coor.~Ascent\",\n",
    "    \"lambda-mart\": r\"LambdaMART\",\n",
    "    \"list-net\": r\"ListNET\",\n",
    "    \"rank-boost\": r\"RankBoost\",\n",
    "    \"linear-regression\": r\"Regression\"\n",
    "}\n",
    "\n",
    "sampling_names = {\n",
    "    (\"identity\", \"identity\", \"identity\"): r\"100\\,\\%\",\n",
    "    (\"no-wikipedia-redundancy\", \"identity\", \"identity\"): r\"0\\,\\%\",\n",
    "    (\"filter-canonical\", \"identity\", \"identity\"): r\"0\\,\\%\",\n",
    "#     (\"identity\", \"identity\", \"novelty-relevance-feedback-null\"): r\"NOV\\textsubscript{0}\",\n",
    "#     (\"identity\", \"identity\", \"novelty-relevance-feedback-null-novelty-feature\"): r\"NOV\\textsubscript{0,F}\",\n",
    "#     (\"identity\", \"identity\", \"novelty-relevance-feedback-scale\"): r\"NOV\\textsubscript{S}\",\n",
    "    (\"identity\", \"identity\", \"novelty-relevance-feedback-scale-novelty-feature\"): r\"NOV\\textsubscript{S,F}\",\n",
    "}\n",
    "\n",
    "split_names = {\n",
    "    \"most-redundant-training\": r\"Worst-Case Scenario\",\n",
    "#     \"3-fold-cross-validation-1\": r\"3-Fold Cross Validation\",\n",
    "#     \"3-fold-cross-validation-2\": r\"3-Fold Cross Validation\",\n",
    "#     \"3-fold-cross-validation-3\": r\"3-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-1\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-2\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-3\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-4\": r\"5-Fold Cross Validation\",\n",
    "    \"5-fold-cross-validation-5\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-mostredundanttraining\": r\"Worst-Case Scenario\",\n",
    "    \"clueweb09-fold1\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold2\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold3\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold4\": r\"5-Fold Cross Validation\",\n",
    "    \"clueweb09-fold5\": r\"5-Fold Cross Validation\",\n",
    "    \"letor-trec-millionquery2007-fold-1\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "    \"letor-trec-millionquery2007-fold-2\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "    \"letor-trec-millionquery2007-fold-3\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "    \"letor-trec-millionquery2007-fold-4\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "    \"letor-trec-millionquery2007-fold-5\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "    \"letor-trec-millionquery2008-fold-1\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "    \"letor-trec-millionquery2008-fold-2\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "    \"letor-trec-millionquery2008-fold-3\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "    \"letor-trec-millionquery2008-fold-4\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "    \"letor-trec-millionquery2008-fold-5\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "    \"trec-millionquery2007-fold1\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2007-fold2\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2007-fold3\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2007-fold4\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2007-fold5\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2008-fold1\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2008-fold2\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2008-fold3\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2008-fold4\": r\"5-Fold Cross Validation\",\n",
    "    \"trec-millionquery2008-fold5\": r\"5-Fold Cross Validation\"\n",
    "#     \"trec-millionquery2007-fold1\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "#     \"trec-millionquery2007-fold2\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "#     \"trec-millionquery2007-fold3\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "#     \"trec-millionquery2007-fold4\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "#     \"trec-millionquery2007-fold5\": r\"5-Fold Cross Validation MQ\\,2007\",\n",
    "#     \"trec-millionquery2008-fold1\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "#     \"trec-millionquery2008-fold2\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "#     \"trec-millionquery2008-fold3\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "#     \"trec-millionquery2008-fold4\": r\"5-Fold Cross Validation MQ\\,2008\",\n",
    "#     \"trec-millionquery2008-fold5\": r\"5-Fold Cross Validation MQ\\,2008\"\n",
    "}\n",
    "\n",
    "evaluations = list(evaluation_names.keys())\n",
    "\n",
    "corpora = list(corpus_names.keys())\n",
    "\n",
    "evaluation_filter_metrics = {\n",
    "    \"ndcg@10-per-topic\": \"ndcg@10\",\n",
    "}\n",
    "evaluation_filter_metrics = { e : evaluation_filter_metrics.get(e, \"ndcg@20\") for e in evaluations }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "baseline_ranker = \"BM25\"\n",
    "baseline_sampling = sampling_names[(\"identity\", \"identity\", \"identity\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse evaluation data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>metric</th>\n",
       "      <th>underSampling</th>\n",
       "      <th>overSampling</th>\n",
       "      <th>featureMutation</th>\n",
       "      <th>trial</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>test-set-result</th>\n",
       "      <th>train-set-result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clueweb09</td>\n",
       "      <td>clueweb09-fold2</td>\n",
       "      <td>rank-net</td>\n",
       "      <td>ndcg@20</td>\n",
       "      <td>filter-canonical</td>\n",
       "      <td>identity</td>\n",
       "      <td>novelty-relevance-feedback-null-novelty-feature</td>\n",
       "      <td>trial-0</td>\n",
       "      <td>remove-duplicates</td>\n",
       "      <td>ndcg@10-per-topic</td>\n",
       "      <td>[0.0, 0.0, 0.09111153494792501, 0.138375429649...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clueweb09</td>\n",
       "      <td>clueweb09-fold2</td>\n",
       "      <td>rank-net</td>\n",
       "      <td>ndcg@20</td>\n",
       "      <td>filter-canonical</td>\n",
       "      <td>identity</td>\n",
       "      <td>novelty-relevance-feedback-null-novelty-feature</td>\n",
       "      <td>trial-0</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>ndcg@10-per-topic</td>\n",
       "      <td>[0.0, 0.0, 0.09111153494792501, 0.138375429649...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23038</th>\n",
       "      <td>gov2</td>\n",
       "      <td>trec-millionquery2008-fold5</td>\n",
       "      <td>list-net</td>\n",
       "      <td>ndcg@10</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>trial-3</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>ndcg@10-per-topic</td>\n",
       "      <td>[0.5, 0.386852807234541, 0.93277838931011, 0.6...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23039</th>\n",
       "      <td>gov2</td>\n",
       "      <td>trec-millionquery2008-fold5</td>\n",
       "      <td>list-net</td>\n",
       "      <td>ndcg@10</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>identity</td>\n",
       "      <td>trial-3</td>\n",
       "      <td>identity</td>\n",
       "      <td>ndcg@10-per-topic</td>\n",
       "      <td>[0.5, 0.386852807234541, 0.93277838931011, 0.8...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23040 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          corpus               trainTestSplit    ranker   metric  \\\n",
       "0      clueweb09              clueweb09-fold2  rank-net  ndcg@20   \n",
       "1      clueweb09              clueweb09-fold2  rank-net  ndcg@20   \n",
       "...          ...                          ...       ...      ...   \n",
       "23038       gov2  trec-millionquery2008-fold5  list-net  ndcg@10   \n",
       "23039       gov2  trec-millionquery2008-fold5  list-net  ndcg@10   \n",
       "\n",
       "          underSampling overSampling  \\\n",
       "0      filter-canonical     identity   \n",
       "1      filter-canonical     identity   \n",
       "...                 ...          ...   \n",
       "23038          identity     identity   \n",
       "23039          identity     identity   \n",
       "\n",
       "                                       featureMutation    trial  \\\n",
       "0      novelty-relevance-feedback-null-novelty-feature  trial-0   \n",
       "1      novelty-relevance-feedback-null-novelty-feature  trial-0   \n",
       "...                                                ...      ...   \n",
       "23038                                         identity  trial-3   \n",
       "23039                                         identity  trial-3   \n",
       "\n",
       "                 runSampling         evaluation  \\\n",
       "0          remove-duplicates  ndcg@10-per-topic   \n",
       "1      duplicates-irrelevant  ndcg@10-per-topic   \n",
       "...                      ...                ...   \n",
       "23038  duplicates-irrelevant  ndcg@10-per-topic   \n",
       "23039               identity  ndcg@10-per-topic   \n",
       "\n",
       "                                         test-set-result  train-set-result  \n",
       "0      [0.0, 0.0, 0.09111153494792501, 0.138375429649...               NaN  \n",
       "1      [0.0, 0.0, 0.09111153494792501, 0.138375429649...               NaN  \n",
       "...                                                  ...               ...  \n",
       "23038  [0.5, 0.386852807234541, 0.93277838931011, 0.6...               NaN  \n",
       "23039  [0.5, 0.386852807234541, 0.93277838931011, 0.8...               NaN  \n",
       "\n",
       "[23040 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from JSON-Lines file.\n",
    "def get_evaluation_raw(name):\n",
    "    file = EVALUATION_DIR / (\"evaluation-of-experiments-\" + name + \".jsonl\")\n",
    "    return pd.read_json(file.open(), lines=True)\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation_raw(evaluations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>test-set-result</th>\n",
       "      <th>sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>clueweb09-fold2</td>\n",
       "      <td>rank-net</td>\n",
       "      <td>remove-duplicates</td>\n",
       "      <td>[0.0, 0.0, 0.09111153494792501, 0.138375429649...</td>\n",
       "      <td>(filter-canonical, identity, novelty-relevance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>clueweb09-fold2</td>\n",
       "      <td>rank-net</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>[0.0, 0.0, 0.09111153494792501, 0.138375429649...</td>\n",
       "      <td>(filter-canonical, identity, novelty-relevance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>clueweb09-fold4</td>\n",
       "      <td>list-net</td>\n",
       "      <td>duplicates-irrelevant</td>\n",
       "      <td>[0.334022806616867, 0.0, 0.38156289940793703, ...</td>\n",
       "      <td>(identity, identity, identity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>clueweb09-fold4</td>\n",
       "      <td>list-net</td>\n",
       "      <td>identity</td>\n",
       "      <td>[0.29131539076245705, 0.0, 0.251687887754597, ...</td>\n",
       "      <td>(identity, identity, identity)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trainTestSplit    ranker            runSampling  \\\n",
       "90    clueweb09-fold2  rank-net      remove-duplicates   \n",
       "91    clueweb09-fold2  rank-net  duplicates-irrelevant   \n",
       "...               ...       ...                    ...   \n",
       "8638  clueweb09-fold4  list-net  duplicates-irrelevant   \n",
       "8639  clueweb09-fold4  list-net               identity   \n",
       "\n",
       "                                        test-set-result  \\\n",
       "90    [0.0, 0.0, 0.09111153494792501, 0.138375429649...   \n",
       "91    [0.0, 0.0, 0.09111153494792501, 0.138375429649...   \n",
       "...                                                 ...   \n",
       "8638  [0.334022806616867, 0.0, 0.38156289940793703, ...   \n",
       "8639  [0.29131539076245705, 0.0, 0.251687887754597, ...   \n",
       "\n",
       "                                               sampling  \n",
       "90    (filter-canonical, identity, novelty-relevance...  \n",
       "91    (filter-canonical, identity, novelty-relevance...  \n",
       "...                                                 ...  \n",
       "8638                     (identity, identity, identity)  \n",
       "8639                     (identity, identity, identity)  \n",
       "\n",
       "[4320 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_evaluation(evaluation_name, corpus=None):\n",
    "    evaluation = get_evaluation_raw(evaluation_name)\n",
    "    \n",
    "    # Drop training set results.\n",
    "    evaluation = evaluation.drop(columns=[\"train-set-result\"])\n",
    "\n",
    "    # Drop evaluation column.\n",
    "    evaluation = evaluation.drop(columns=[\"evaluation\"])\n",
    "\n",
    "    # Drop trial column.\n",
    "    evaluation = evaluation.drop(columns=[\"trial\"])\n",
    "\n",
    "    # Filter corpus.\n",
    "    if corpus:\n",
    "        evaluation = evaluation[evaluation[\"corpus\"] == corpus]\\\n",
    "            .drop(columns=[\"corpus\"])\n",
    "\n",
    "    # Filter models with metric.\n",
    "    filter_metric = evaluation_filter_metrics[evaluation_name]\n",
    "    evaluation = evaluation[evaluation[\"metric\"] == filter_metric]\\\n",
    "        .drop(columns=[\"metric\"])\n",
    "\n",
    "    # Merge samplings into one column.\n",
    "    evaluation[\"sampling\"] = evaluation[[\"underSampling\",\"overSampling\",\"featureMutation\"]]\\\n",
    "        .aggregate(tuple, axis=1)\n",
    "    evaluation = evaluation.drop(columns=[\"underSampling\",\"overSampling\",\"featureMutation\"])\n",
    "\n",
    "    return evaluation\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation(evaluations[0], corpora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>test-set-result</th>\n",
       "      <th>sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Removed</td>\n",
       "      <td>[0.562913917456436, 0.0, 0.023363055231470004,...</td>\n",
       "      <td>0\\,\\%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Irrelevant</td>\n",
       "      <td>[0.562913917456436, 0.0, 0.023363055231470004,...</td>\n",
       "      <td>0\\,\\%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>ListNET</td>\n",
       "      <td>Duplicates Irrelevant</td>\n",
       "      <td>[0.334022806616867, 0.0, 0.38156289940793703, ...</td>\n",
       "      <td>100\\,\\%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>ListNET</td>\n",
       "      <td>Duplicates Unmodified</td>\n",
       "      <td>[0.29131539076245705, 0.0, 0.251687887754597, ...</td>\n",
       "      <td>100\\,\\%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1890 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               trainTestSplit      ranker            runSampling  \\\n",
       "300   5-Fold Cross Validation  Regression     Duplicates Removed   \n",
       "301   5-Fold Cross Validation  Regression  Duplicates Irrelevant   \n",
       "...                       ...         ...                    ...   \n",
       "8638  5-Fold Cross Validation     ListNET  Duplicates Irrelevant   \n",
       "8639  5-Fold Cross Validation     ListNET  Duplicates Unmodified   \n",
       "\n",
       "                                        test-set-result sampling  \n",
       "300   [0.562913917456436, 0.0, 0.023363055231470004,...    0\\,\\%  \n",
       "301   [0.562913917456436, 0.0, 0.023363055231470004,...    0\\,\\%  \n",
       "...                                                 ...      ...  \n",
       "8638  [0.334022806616867, 0.0, 0.38156289940793703, ...  100\\,\\%  \n",
       "8639  [0.29131539076245705, 0.0, 0.251687887754597, ...  100\\,\\%  \n",
       "\n",
       "[1890 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_evaluation_labeled(evaluation_name, corpus=None):\n",
    "    evaluation = get_evaluation(evaluation_name, corpus)\n",
    "\n",
    "    # Map names.\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        evaluation[\"corpus\"] = evaluation[\"corpus\"].map(lambda split : corpus_names.get(split, \"\"))\n",
    "    evaluation[\"trainTestSplit\"] = evaluation[\"trainTestSplit\"].map(lambda split : split_names.get(split, \"\"))\n",
    "    evaluation[\"ranker\"] = evaluation[\"ranker\"].map(lambda ranker : ranker_names.get(ranker, \"\"))\n",
    "    evaluation[\"runSampling\"] = evaluation[\"runSampling\"].map(lambda run_sampling : run_sampling_names.get(run_sampling, \"\"))\n",
    "    evaluation[\"sampling\"] = evaluation[\"sampling\"].map(lambda sampling : sampling_names.get(sampling, \"\"))\n",
    "\n",
    "    # Filter empty (ignored) names.\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        evaluation=evaluation[evaluation[\"corpus\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"trainTestSplit\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"ranker\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"runSampling\"] != \"\"]\n",
    "    evaluation=evaluation[evaluation[\"sampling\"] != \"\"]\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation_labeled(evaluations[0], corpora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainTestSplit</th>\n",
       "      <th>ranker</th>\n",
       "      <th>runSampling</th>\n",
       "      <th>sampling</th>\n",
       "      <th>test-set-result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Worst-Case Scenario</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Duplicates Unmodified</td>\n",
       "      <td>100\\,\\%</td>\n",
       "      <td>[0.06968312249684201, 0.0, 0.140698320123606, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst-Case Scenario</td>\n",
       "      <td>BM25</td>\n",
       "      <td>Duplicates Unmodified</td>\n",
       "      <td>0\\,\\%</td>\n",
       "      <td>[0.06968312249684201, 0.0, 0.140698320123606, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Removed</td>\n",
       "      <td>0\\,\\%</td>\n",
       "      <td>[0.562913917456436, 0.0, 0.023363055231470004,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>5-Fold Cross Validation</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Duplicates Removed</td>\n",
       "      <td>NOV\\textsubscript{S,F}</td>\n",
       "      <td>[0.253991566674711, 0.0, 0.009088684028421001,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              trainTestSplit      ranker            runSampling  \\\n",
       "0        Worst-Case Scenario        BM25  Duplicates Unmodified   \n",
       "1        Worst-Case Scenario        BM25  Duplicates Unmodified   \n",
       "..                       ...         ...                    ...   \n",
       "124  5-Fold Cross Validation  Regression     Duplicates Removed   \n",
       "125  5-Fold Cross Validation  Regression     Duplicates Removed   \n",
       "\n",
       "                   sampling                                    test-set-result  \n",
       "0                   100\\,\\%  [0.06968312249684201, 0.0, 0.140698320123606, ...  \n",
       "1                     0\\,\\%  [0.06968312249684201, 0.0, 0.140698320123606, ...  \n",
       "..                      ...                                                ...  \n",
       "124                   0\\,\\%  [0.562913917456436, 0.0, 0.023363055231470004,...  \n",
       "125  NOV\\textsubscript{S,F}  [0.253991566674711, 0.0, 0.009088684028421001,...  \n",
       "\n",
       "[126 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorical_type(categories):\n",
    "    categories = list(categories)\n",
    "    categories = sorted(set(categories), key=categories.index)\n",
    "    return pd.api.types.CategoricalDtype(categories=categories, ordered=True)\n",
    "\n",
    "# Categories:\n",
    "corpus_categorical_type = categorical_type(corpus_names.values())\n",
    "split_categorical_type = categorical_type(split_names.values())\n",
    "ranker_categorical_type = categorical_type(ranker_names.values())\n",
    "run_sampling_categorical_type = categorical_type(run_sampling_names.values())\n",
    "sampling_categorical_type = categorical_type(sampling_names.values())\n",
    "\n",
    "def get_evaluation_aggregated(evaluation_name, corpus=None):\n",
    "    evaluation = get_evaluation_labeled(evaluation_name, corpus)\n",
    "\n",
    "    # Make types categorical.\n",
    "    types = {\n",
    "        \"trainTestSplit\": split_categorical_type,\n",
    "        \"ranker\": ranker_categorical_type,\n",
    "        \"runSampling\": run_sampling_categorical_type,\n",
    "        \"sampling\": sampling_categorical_type\n",
    "    }\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        types.update({\"corpus\" : corpus_categorical_type})\n",
    "    evaluation = evaluation.astype(types)\n",
    "    \n",
    "    # Sort.\n",
    "    sort_cols = [\"trainTestSplit\", \"ranker\", \"runSampling\", \"sampling\"]\n",
    "    if \"corpus\" in evaluation.columns:\n",
    "        sort_cols.insert(0, \"corpus\")\n",
    "    evaluation = evaluation.sort_values(by=sort_cols)\n",
    "\n",
    "    # Aggregate trials.\n",
    "    evaluation = evaluation.groupby(sort_cols)\\\n",
    "        .aggregate(lambda lists : [item for sublist in lists for item in sublist])\\\n",
    "        .dropna()\\\n",
    "        .reset_index()\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Only print for debugging.\n",
    "get_evaluation_aggregated(evaluations[0], corpora[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_P_VALUE = 0.05\n",
    "\n",
    "def significantly_better(compare, baseline):\n",
    "    test = ttest_ind(compare,baseline)\n",
    "    return test.statistic > 0 and test.pvalue <= MAX_P_VALUE\n",
    "\n",
    "def cohens_d(compare, baseline):\n",
    "    return (mean(compare) - mean(baseline)) / (sqrt((stdev(compare) ** 2 + stdev(baseline) ** 2) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LaTeX table from data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_columns(n):\n",
    "    return [\"\"] * n\n",
    "\n",
    "def table(name, corpus=None, decimals=3):\n",
    "    evaluation = get_evaluation_aggregated(name, corpus)\n",
    "    \n",
    "    rankers = evaluation[\"ranker\"].unique()\n",
    "    run_samplings = evaluation[\"runSampling\"].unique()\n",
    "    samplings = evaluation[\"sampling\"].unique()\n",
    "\n",
    "\n",
    "    def table_head():\n",
    "        if not corpus:\n",
    "            head = [\"Corpus\", \"Split\", \"Algorithm\"]\n",
    "        else: \n",
    "            head = [\"Split\", \"Algorithm\"]\n",
    "        head.append(evaluation_names[name])\n",
    "        head += empty_columns(len(samplings) * len(run_samplings) - 1)\n",
    "        head = list(map(lambda item : r\"\\textbf{\" + item + r\"}\" if len(item) > 0 else item, head))\n",
    "        return head\n",
    "    \n",
    "    \n",
    "    def table_subhead():\n",
    "        head = empty_columns(3 if not corpus else 2)\n",
    "        for run_sampling in run_samplings:\n",
    "            head.append(run_sampling)\n",
    "            head += empty_columns(len(samplings) - 1)\n",
    "        return head\n",
    "    \n",
    "    \n",
    "    def table_subsubhead():\n",
    "        head = empty_columns(3 if not corpus else 2)\n",
    "        for _ in run_samplings:\n",
    "            for sampling in samplings:\n",
    "                head.append(sampling)\n",
    "        return head\n",
    "\n",
    "\n",
    "    def table_cell(baseline, compare):\n",
    "        column = r\"\\(\"\n",
    "\n",
    "        significant = significantly_better(compare, baseline)\n",
    "        if significant:\n",
    "            column += r\"\\mathbf{\"\n",
    "\n",
    "        column += (\"{:.\" + str(decimals) + \"f}\").format(mean(compare))\n",
    "\n",
    "        d = cohens_d(compare, baseline)\n",
    "        if d > 0:\n",
    "            column += r\"\\updiff{\"\n",
    "            column += \"{:.1f}\".format(d)\n",
    "            column += r\"}\"\n",
    "        elif d < 0:\n",
    "            column += r\"\\downdiff{\"\n",
    "            column += \"{:.1f}\".format(-d)\n",
    "            column += r\"}\"\n",
    "        else:\n",
    "            column += r\"\\nodiff{\"\n",
    "            column += \"{:.1f}\".format(d)\n",
    "            column += r\"}\"\n",
    "\n",
    "        if significant:\n",
    "            column += r\"}\"\n",
    "\n",
    "        column += r\"\\)\"\n",
    "        return column\n",
    "\n",
    "\n",
    "    def table_row(split, split_tex, ranker, row_corpus=None):\n",
    "        if row_corpus:\n",
    "            row = [row_corpus, split_tex, ranker]\n",
    "        else:\n",
    "            row = [split_tex, ranker]\n",
    "        for run_sampling in run_samplings:\n",
    "            df = evaluation\n",
    "            if row_corpus:\n",
    "                df = df[df[\"corpus\"] == row_corpus]\n",
    "            df = df[df[\"trainTestSplit\"] == split]\n",
    "            df = df[df[\"ranker\"] == ranker]\n",
    "            df = df[df[\"runSampling\"] == run_sampling]\n",
    "            if row_corpus:\n",
    "                drop_columns = [\"corpus\", \"trainTestSplit\", \"ranker\", \"runSampling\"]\n",
    "            else: \n",
    "                drop_columns = [\"trainTestSplit\", \"ranker\", \"runSampling\"]\n",
    "            df = df.drop(columns=drop_columns)\n",
    "            baseline_result = df[df[\"sampling\"] == baseline_sampling][\"test-set-result\"].iloc[0]\n",
    "            \n",
    "            row.append(r\"\\(\" + (\"{:.\" + str(decimals) + \"f}\").format(mean(baseline_result)) + r\"\\)\")\n",
    "            for sampling in samplings:\n",
    "                if sampling != baseline_sampling:\n",
    "                    if ranker == baseline_ranker:\n",
    "                        # We don't see sampling differences in BM25 Ranking, \n",
    "                        # as those don't depend on training data.\n",
    "                        # Therefore hide all except the first.\n",
    "                        row.append(r\"---\")\n",
    "                    else:\n",
    "                        compare_result = df[df[\"sampling\"] == sampling][\"test-set-result\"].iloc[0]\n",
    "                        row.append(table_cell(baseline_result, compare_result))\n",
    "        return row\n",
    "\n",
    "\n",
    "    def table_rows():\n",
    "        def split_rotated(split_name, num_rankers):\n",
    "            return r\"\\multirow{\" + str(num_rankers) +\\\n",
    "                r\"}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{\" +\\\n",
    "                str(num_rankers + 1) +\\\n",
    "                r\"em}{\\centering \\textbf{\" + split_name + \"}}}}\"\n",
    "        \n",
    "        rows = []\n",
    "        if not corpus:\n",
    "            for corp in evaluation[\"corpus\"].unique():\n",
    "                corpus_df = evaluation[evaluation[\"corpus\"] == corp]\n",
    "                for split in corpus_df[\"trainTestSplit\"].unique():\n",
    "                    split_tex = split_rotated(split, len(rankers))\n",
    "                    for ranker in rankers:\n",
    "                        rows.append(table_row(split, split_tex, ranker, corp))\n",
    "                        split_tex = \"\"\n",
    "        else:\n",
    "            for split in evaluation[\"trainTestSplit\"].unique():\n",
    "                split_tex = split_rotated(split, len(rankers))\n",
    "                for ranker in rankers:\n",
    "                    rows.append(table_row(split, split_tex, ranker))\n",
    "                    split_tex = \"\"\n",
    "        return rows\n",
    "\n",
    "\n",
    "    table_data = [\n",
    "        table_head(),\n",
    "        table_subhead(),\n",
    "        table_subsubhead()\n",
    "    ] + table_rows()\n",
    "\n",
    "    return tabulate(table_data, tablefmt=\"latex_raw\")\n",
    "\n",
    "def write_table(evaluation, corpus=None, decimals=3):\n",
    "    file_name = OUTPUT_DIR / (corpus + (\"-\" if corpus else \"\") + evaluation + \".tex\")\n",
    "    with codecs.open(file_name, 'w', 'utf-8') as file:\n",
    "        content = table(evaluation, corpus, decimals)\n",
    "        content = re.sub(r\"\\s+&\\s+\", \" & \",content)\n",
    "        content = re.sub(r\"\\s+\\\\\\\\\", r\" \\\\\\\\\",content)\n",
    "        file.write(r\"\\documentclass[preview]{standalone}\" + \"\\n\" +\\\n",
    "                   r\"\\usepackage{amsmath}\" + \"\\n\" +\\\n",
    "                   r\"\\usepackage{graphicx}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\ndcg}[1]{nDCG\\def\\tempndcg{#1}\\ifx\\tempndcg\\empty\\else{@}\\tempndcg\\fi}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\map}{MAP}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\updiff}[1]{^{\\text{↑}#1}}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\downdiff}[1]{^{\\text{↓}#1}}\" + \"\\n\" +\\\n",
    "                   r\"\\newcommand{\\nodiff}[1]{^{\\text{=}#1}}\" + \"\\n\" +\\\n",
    "                   r\"\\begin{document}\" + \"\\n\")\n",
    "        file.write(content)\n",
    "        file.write(r\"\\end{document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table(\"domain-fairness-per-topic\")\n",
    "write_table(\"domain-fairness-per-topic\", corpus=\"gov2\")\n",
    "write_table(\"domain-fairness-per-topic\", corpus=\"clueweb09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table(\"map-per-topic\")\n",
    "# write_table(\"map-per-topic\", corpus=\"gov2\")\n",
    "# write_table(\"map-per-topic\", corpus=\"clueweb09\")\n",
    "# write_table(\"ndcg@10-per-topic\")\n",
    "write_table(\"ndcg@10-per-topic\", corpus=\"gov2\")\n",
    "write_table(\"ndcg@10-per-topic\", corpus=\"clueweb09\")\n",
    "# write_table(\"ndcg@20-per-topic\")\n",
    "write_table(\"ndcg@20-per-topic\", corpus=\"gov2\")\n",
    "write_table(\"ndcg@20-per-topic\", corpus=\"clueweb09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_table(\"first-wikipedia-rank-per-topic\", decimals=0, corpus=\"clueweb09\")\n",
    "write_table(\"first-irrelevant-wikipedia-rank-per-topic\", decimals=0, corpus=\"clueweb09\")\n",
    "# write_table(\"first-duplicate-rank-per-topic\", decimals=0)\n",
    "# write_table(\"first-duplicate-rank-per-topic\", decimals=0, corpus=\"gov2\")\n",
    "# write_table(\"first-duplicate-rank-per-topic\", decimals=0, corpus=\"clueweb09\")\n",
    "# write_table(\"first-irrelevant-duplicate-rank-per-topic\", decimals=0)\n",
    "write_table(\"first-irrelevant-duplicate-rank-per-topic\", decimals=0, corpus=\"gov2\")\n",
    "write_table(\"first-irrelevant-duplicate-rank-per-topic\", decimals=0, corpus=\"clueweb09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
